{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fortune 500 Web scrape (Selenium and BeautifulSoup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scrape Initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_headers(soup):\n",
    "\n",
    "    table = soup.find('div', class_=\"ReactTable\")\n",
    "\n",
    "    headers = []\n",
    "    # Class is unique is constant throughout the html for ALL headings.\n",
    "    theaders = table.find_all('div', class_='searchResults__columnTitle--1Brf4')\n",
    "    for header in theaders:\n",
    "        headers.append(header.text)\n",
    "#         print(header.text)\n",
    "\n",
    "    with open ('Fortune500.csv','w') as r:\n",
    "        for col in headers:\n",
    "            # Prevents additional columns being creating, due to csv format.\n",
    "            if ',' in col:\n",
    "                col = col.replace(',','')\n",
    "            r.write(col)\n",
    "            # Prevents the creation of an extra column.\n",
    "            if col != headers[-1]:\n",
    "                r.write(',')\n",
    "        r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(soup):\n",
    "\n",
    "    table = soup.find('div', class_=\"ReactTable\")\n",
    "    tbody = table.find('div', class_=\"rt-tbody\")\n",
    "    rows = tbody.find_all('div', class_='rt-tr-group')\n",
    "\n",
    "    with open ('Fortune500.csv','a') as r:        \n",
    "        for row in rows:\n",
    "            # Class is unique is constant throughout the html for ALL data points.\n",
    "            cols = row.find_all('div', class_='searchResults__cellContent--3WEWj')\n",
    "            for col in cols:\n",
    "                value = col.text\n",
    "                # Prevents additional columns being creating, due to csv format.\n",
    "                if ',' in value:\n",
    "                    value = value.replace(',','')\n",
    "                r.write(value)\n",
    "                r.write(',')\n",
    "            r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Scraping Page: 1\nScraping Page: 2\nScraping Page: 3\nScraping Page: 4\nScraping Page: 5\nScraping Page: 6\nScraping Page: 7\nScraping Page: 8\nScraping Page: 9\nScraping Page: 10\nPages scraped: 10\nComplete\n"
    }
   ],
   "source": [
    "path = \"/Users/hongbinlin/Downloads/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "webpage = \"https://fortune.com/fortune500/2020/search/\"\n",
    "driver.get(webpage)\n",
    "\n",
    "try:\n",
    "    # This is important to ensure that the webpage has rendered.\n",
    "    table = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"rt-tbody\")))\n",
    "    \n",
    "    # Show 100 rows\n",
    "    select = Select(driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[2]/div/div[2]/div/div[2]/span[2]/select'))\n",
    "    select.select_by_value('100')\n",
    "    \n",
    "    page = 1\n",
    "    while page <= 10:    \n",
    "        print(\"Scraping Page: {}\".format(page))\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        if page == 1:\n",
    "            create_headers(soup)\n",
    "        scrape_data(soup)\n",
    "        \n",
    "        show_more = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"-next\")))\n",
    "        show_more.click()\n",
    "        page += 1\n",
    "\n",
    "\n",
    "    print(\"Pages scraped: {}\".format(page-1))\n",
    "    print('Complete')\n",
    "\n",
    "except NoSuchElementException:\n",
    "    print(\"Table not found. Closing application.\")\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Fortune500.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     Rank                       name Revenues ($M) Revenue Percent Change  \\\n995   996           Mr. Cooper Group         $2007                      -   \n996   997              Herc Holdings         $1999                   1.1%   \n997   998      Healthpeak Properties       $1997.4                   8.2%   \n998   999                   SPX FLOW       $1996.3                  -4.5%   \n999  1000  Liberty Oilfield Services       $1990.3                  -7.6%   \n\n    Profits ($M) Profits Percent Change Assets ($M)  \\\n995         $274                      -      $18305   \n996        $47.5                 -31.3%       $3817   \n997        $45.5                 -95.7%    $14032.9   \n998       $-95.1                -316.1%     $2437.4   \n999          $39                 -69.2%     $1283.4   \n\n    Market Value — as of March 31 2020 ($M) Change in Rank (Full 1000)  \\\n995                                  $674.1                          -   \n996                                  $590.5                         -4   \n997                                $12059.3                          -   \n998                                 $1211.8                        -37   \n999                                  $302.8                        -58   \n\n     Employees Change in Rank (500 only)  \n995       9100                         -  \n996       5100                         -  \n997        204                         -  \n998       5000                         -  \n999       2571                         -  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rank</th>\n      <th>name</th>\n      <th>Revenues ($M)</th>\n      <th>Revenue Percent Change</th>\n      <th>Profits ($M)</th>\n      <th>Profits Percent Change</th>\n      <th>Assets ($M)</th>\n      <th>Market Value — as of March 31 2020 ($M)</th>\n      <th>Change in Rank (Full 1000)</th>\n      <th>Employees</th>\n      <th>Change in Rank (500 only)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>995</th>\n      <td>996</td>\n      <td>Mr. Cooper Group</td>\n      <td>$2007</td>\n      <td>-</td>\n      <td>$274</td>\n      <td>-</td>\n      <td>$18305</td>\n      <td>$674.1</td>\n      <td>-</td>\n      <td>9100</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>997</td>\n      <td>Herc Holdings</td>\n      <td>$1999</td>\n      <td>1.1%</td>\n      <td>$47.5</td>\n      <td>-31.3%</td>\n      <td>$3817</td>\n      <td>$590.5</td>\n      <td>-4</td>\n      <td>5100</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>998</td>\n      <td>Healthpeak Properties</td>\n      <td>$1997.4</td>\n      <td>8.2%</td>\n      <td>$45.5</td>\n      <td>-95.7%</td>\n      <td>$14032.9</td>\n      <td>$12059.3</td>\n      <td>-</td>\n      <td>204</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>999</td>\n      <td>SPX FLOW</td>\n      <td>$1996.3</td>\n      <td>-4.5%</td>\n      <td>$-95.1</td>\n      <td>-316.1%</td>\n      <td>$2437.4</td>\n      <td>$1211.8</td>\n      <td>-37</td>\n      <td>5000</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>1000</td>\n      <td>Liberty Oilfield Services</td>\n      <td>$1990.3</td>\n      <td>-7.6%</td>\n      <td>$39</td>\n      <td>-69.2%</td>\n      <td>$1283.4</td>\n      <td>$302.8</td>\n      <td>-58</td>\n      <td>2571</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get additional data columns from company specific website on Fortune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_header(rows,financials):\n",
    "    with open ('Fortune500-2.csv','w') as r:\n",
    "        for row in rows:\n",
    "            data = row.find_all('div')\n",
    "            heading = data[0].text\n",
    "            r.write(heading)\n",
    "            r.write(',')\n",
    "        for financial in financials:\n",
    "            heading = financial.find('div').text\n",
    "            if ',' in heading:\n",
    "                heading = heading.replace(',','')\n",
    "            r.write(heading)\n",
    "            r.write(',')\n",
    "        r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_data(rows,financials):\n",
    "     with open ('Fortune500-2.csv','a') as r:\n",
    "            for row in rows:\n",
    "                data = row.find_all('div')\n",
    "                heading = data[0].text\n",
    "                if heading == 'Website':\n",
    "                    col = row.a.text\n",
    "                else:\n",
    "                    col = data[-1].text\n",
    "\n",
    "                if ',' in col:\n",
    "                    col = col.replace(',','')\n",
    "                r.write(col)\n",
    "                r.write(',')\n",
    "                \n",
    "            for financial in financials:\n",
    "                value = financial.find('div', class_='dataTable__value--2wIAD').text\n",
    "                if ',' in value:\n",
    "                    value = value.replace(',','')\n",
    "                r.write(value)\n",
    "                r.write(',')\n",
    "            r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_failed_scrapes(failed_scrapes):\n",
    "    '''\n",
    "    This function shouldn't run with the new method. i.e. extracting url from 'next' pagination, instead of 'guessing' url from company name.\n",
    "    AT&T is just ATT,\n",
    "    Amazon.com is just Amazon\n",
    "    Tapestry is actually Coach'''\n",
    "\n",
    "    if os.path.exists('Fortune500-2-failed.csv'):\n",
    "        with open ('Fortune500-2-failed.csv','a') as r:\n",
    "            for company in failed_scrapes:\n",
    "                r.write(company)\n",
    "                r.write('\\n')\n",
    "        \n",
    "    else:\n",
    "        with open ('Fortune500-2-failed.csv','w') as r:\n",
    "            for company in failed_scrapes:\n",
    "                r.write(company)\n",
    "                r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This method would be a lot faster if we store the urls in a list, and then use BS4 in future to extract \n",
    "directly from the html, as opposed to using Selenium to render each page. \n",
    "Sadly, we need the page to render to get the next url...\n",
    "However, this is still a better solution than just 'guessing' the url. It makes the method a lot more \n",
    "robust for changes in the different years.\n",
    "'''\n",
    "\n",
    "path = \"/Users/hongbinlin/Downloads/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "try:\n",
    "    failed_scrapes = []\n",
    "    company = 'Walmart'\n",
    "    webpage = \"https://fortune.com/company/{}/fortune500/\".format(company)\n",
    "    \n",
    "# This range will scrape n number of pages(max = 1000), \n",
    "# given that company variable is Walmart ie. 1st position\n",
    "    for i in range(1000):\n",
    "#         print(webpage)\n",
    "        driver.get(webpage)\n",
    "\n",
    "        # This is important to ensure that the webpage loads all of the data\n",
    "        try:\n",
    "            temp = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"info__wrapper--1CxpW\")))\n",
    "\n",
    "        except:\n",
    "#             This won't run as company variable is no longer in a list\n",
    "#             failed_scrapes.append(company)\n",
    "            continue\n",
    "            \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        information = soup.find('div', class_='info__wrapper--1CxpW')\n",
    "        rows = information.find_all('div', class_='info__row--7f9lE')\n",
    "        financials = soup.find_all('div', class_='dataTable__row--3ws_o')\n",
    "\n",
    "        if i == 0:\n",
    "            initialise_header(rows,financials)\n",
    "        \n",
    "        scrape_page_data(rows,financials)\n",
    "\n",
    "        pagination = soup.find('div', class_='companySinglePagination__paginationWrapper--2m5Dj')\n",
    "        urls = pagination.find_all('a', href=True)\n",
    "        next_page = urls[-1]['href']\n",
    "        webpage = next_page\n",
    "        \n",
    "finally:\n",
    "    driver.quit()\n",
    "#     store_failed_scrapes(failed_scrapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollar_cols = ['Revenues ($M)',\n",
    "               'Profits ($M)', \n",
    "               'Assets ($M)',\n",
    "               'Market Value — as of March 31 2020 ($M)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Revenues ($M)\nProfits ($M)\nAssets ($M)\nMarket Value — as of March 31 2020 ($M)\n"
    }
   ],
   "source": [
    "for col in dollar_cols:\n",
    "    try:\n",
    "        df[col] = df[col].str.replace('-','0')\n",
    "        df[col] = df[col].str.replace('$','').astype('float')\n",
    "        print(col)\n",
    "    except:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596559928993"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}