{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fortune 500 Web scrape (Selenium and BeautifulSoup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scrape Initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_headers(soup):\n",
    "\n",
    "    table = soup.find('div', class_=\"ReactTable\")\n",
    "\n",
    "    headers = []\n",
    "    # Class is unique is constant throughout the html for ALL headings.\n",
    "    theaders = table.find_all('div', class_='searchResults__columnTitle--1Brf4')\n",
    "    for header in theaders:\n",
    "        headers.append(header.text)\n",
    "    print(headers)\n",
    "    with open ('Fortune500.csv','w') as r:\n",
    "        for col in headers:\n",
    "            # Prevents additional columns being creating, due to csv format.\n",
    "            if ',' in col:\n",
    "                col = col.replace(',','')\n",
    "            r.write(col)\n",
    "            # # Prevents the creation of an extra column.\n",
    "            if col != headers[-1]:\n",
    "                r.write(',')\n",
    "        r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(soup):\n",
    "\n",
    "    table = soup.find('div', class_=\"ReactTable\")\n",
    "    tbody = table.find('div', class_=\"rt-tbody\")\n",
    "    rows = tbody.find_all('div', class_='rt-tr-group')\n",
    "\n",
    "    with open ('Fortune500.csv','a') as r:        \n",
    "        for row in rows:\n",
    "            # Class is unique is constant throughout the html for ALL data points.\n",
    "            cols = row.find_all('div', class_='searchResults__cellContent--3WEWj')\n",
    "            for col in cols:\n",
    "                value = col.text\n",
    "                # Prevents additional columns being creating, due to csv format.\n",
    "                if ',' in value:\n",
    "                    value = value.replace(',','')\n",
    "                r.write(value)\n",
    "                r.write(',')\n",
    "            r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/hongbinlin/Downloads/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "webpage = \"https://fortune.com/fortune500/2020/search/\"\n",
    "driver.get(webpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # This is important to ensure that the webpage has rendered.\n",
    "    table = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"rt-tbody\")))\n",
    "    print(table)\n",
    "    # Show 100 rows\n",
    "    select = Select(driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[2]/div/div[2]/div/div[2]/span[2]/select'))\n",
    "    select.select_by_value('100')\n",
    "    \n",
    "    page = 1\n",
    "    while page <= 10:    \n",
    "        print(\"Scraping Page: {}\".format(page))\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        if page == 1:\n",
    "            create_headers(soup)\n",
    "        scrape_data(soup)\n",
    "        \n",
    "        show_more = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"-next\")))\n",
    "        show_more.click()\n",
    "        page += 1\n",
    "\n",
    "\n",
    "    print(\"Pages scraped: {}\".format(page-1))\n",
    "    print('Complete')\n",
    "\n",
    "except NoSuchElementException:\n",
    "    print(\"Table not found. Closing application.\")\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Fortune500.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get additional data columns from company specific website on Fortune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_header(rows,financials):\n",
    "    with open ('Fortune500-2.csv','w') as r:\n",
    "        for row in rows:\n",
    "            data = row.find_all('div')\n",
    "            heading = data[0].text\n",
    "            r.write(heading)\n",
    "            r.write(',')\n",
    "        for financial in financials:\n",
    "            heading = financial.find('div').text\n",
    "            if ',' in heading:\n",
    "                heading = heading.replace(',','')\n",
    "            r.write(heading)\n",
    "            r.write(',')\n",
    "        r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_data(rows,financials):\n",
    "     with open ('Fortune500-2.csv','a') as r:\n",
    "            for row in rows:\n",
    "                data = row.find_all('div')\n",
    "                heading = data[0].text\n",
    "                if heading == 'Website':\n",
    "                    col = row.a.text\n",
    "                else:\n",
    "                    col = data[-1].text\n",
    "\n",
    "                if ',' in col:\n",
    "                    col = col.replace(',','')\n",
    "                r.write(col)\n",
    "                r.write(',')\n",
    "                \n",
    "            for financial in financials:\n",
    "                value = financial.find('div', class_='dataTable__value--2wIAD').text\n",
    "                if ',' in value:\n",
    "                    value = value.replace(',','')\n",
    "                r.write(value)\n",
    "                r.write(',')\n",
    "            r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_failed_scrapes(failed_scrapes):\n",
    "\n",
    "\n",
    "    '''This function shouldn't run with the new method. i.e. extracting url from 'next' pagination, instead of 'guessing' url from company         name. AT&T is just ATT, Amazon.com is just Amazon Tapestry is actually Coach'''\n",
    "\n",
    "    if os.path.exists('Fortune500-2-failed.csv'):\n",
    "        with open ('Fortune500-2-failed.csv','a') as r:\n",
    "            for company in failed_scrapes:\n",
    "                r.write(company)\n",
    "                r.write('\\n')\n",
    "        \n",
    "    else:\n",
    "        with open ('Fortune500-2-failed.csv','w') as r:\n",
    "            for company in failed_scrapes:\n",
    "                r.write(company)\n",
    "                r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This method would be a lot faster if we store the urls in a list, and then use BS4 in future to extract \n",
    "directly from the html, as opposed to using Selenium to render each page. \n",
    "Sadly, we need the page to render to get the next url...\n",
    "However, this is still a better solution than just 'guessing' the url. It makes the method a lot more \n",
    "robust for changes in the different years.\n",
    "'''\n",
    "\n",
    "path = \"/Users/hongbinlin/Downloads/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "try:\n",
    "    failed_scrapes = []\n",
    "    company = 'Walmart'\n",
    "    webpage = \"https://fortune.com/company/{}/fortune500/\".format(company)\n",
    "    \n",
    "# This range will scrape n number of pages(max = 1000), \n",
    "# given that company variable is Walmart ie. 1st position\n",
    "    for i in range(10):\n",
    "#         print(webpage)\n",
    "        driver.get(webpage)\n",
    "\n",
    "        # This is important to ensure that the webpage loads all of the data\n",
    "        try:\n",
    "            temp = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"info__wrapper--1CxpW\")))\n",
    "\n",
    "        except:\n",
    "#             This won't run as company variable is no longer in a list\n",
    "#             failed_scrapes.append(company)\n",
    "            continue\n",
    "            \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        information = soup.find('div', class_='info__wrapper--1CxpW')\n",
    "        rows = information.find_all('div', class_='info__row--7f9lE')\n",
    "        financials = soup.find_all('div', class_='dataTable__row--3ws_o')\n",
    "\n",
    "        if i == 0:\n",
    "            initialise_header(rows,financials)\n",
    "        \n",
    "        scrape_page_data(rows,financials)\n",
    "\n",
    "        pagination = soup.find('div', class_='companySinglePagination__paginationWrapper--2m5Dj')\n",
    "        urls = pagination.find_all('a', href=True)\n",
    "        next_page = urls[-1]['href']\n",
    "        webpage = next_page\n",
    "        \n",
    "finally:\n",
    "    driver.quit()\n",
    "#     store_failed_scrapes(failed_scrapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollar_cols = ['Revenues ($M)',\n",
    "               'Profits ($M)', \n",
    "               'Assets ($M)',\n",
    "               'Market Value â€” as of March 31 2020 ($M)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in dollar_cols:\n",
    "    try:\n",
    "        df[col] = df[col].str.replace('-','0')\n",
    "        df[col] = df[col].str.replace('$','').astype('float')\n",
    "        print(col)\n",
    "    except:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}