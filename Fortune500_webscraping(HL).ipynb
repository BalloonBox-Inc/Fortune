{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fortune 500 Web scrape (Selenium and BeautifulSoup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ceoFounder():\n",
    "    driver.find_element_by_xpath(\"//label[@for='ceofounder']\").click()\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def femaleCEO():\n",
    "    driver.find_element_by_xpath(\"//label[@for='ceowoman']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageTurner():\n",
    "    driver.find_element_by_xpath(\"//div[@class='-next']\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scrape Initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/hongbinlin/Downloads/chromedriver\"\n",
    "driver = webdriver.Chrome(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def web_scraping(func=0):\n",
    "    # Initial the dataframe\n",
    "    columns =['Rank','Name','Revenue ($M)','Revenue % change','Profit ($M)','Profit % change',\\\n",
    "          'Assets ($M)','Market Value ($M)','Change in rank (1000)','Employees','Change in rank (500)',\\\n",
    "          'Year']\n",
    "    data_list=[]\n",
    "\n",
    "    for year in range(2017,2021): # should be 2017 - 2021(2017 for testing)\n",
    "\n",
    "        # Activate the Chrome Web Driver\n",
    "        URL = 'https://fortune.com/fortune500/{}/search'.format(year)\n",
    "        driver.get(URL)\n",
    "\n",
    "        # Wait until the page is loaded\n",
    "        wait = WebDriverWait(driver,15)\n",
    "        element = wait.until(EC.presence_of_element_located((By.CLASS_NAME,\"ReactTable\")))\n",
    "\n",
    "        # Apply Filter\n",
    "        if (year == 2017) and (func!=0):\n",
    "            continue\n",
    "        else:\n",
    "            if func==femaleCEO:\n",
    "                femaleCEO()\n",
    "            elif func==ceoFounder:\n",
    "                ceoFounder()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        # Set the number of rows in each page\n",
    "        select = Select(driver.find_element_by_xpath(\"//select[@aria-label='rows per page']\"))\n",
    "        select.select_by_value('100')\n",
    "\n",
    "        # Locate the number of page\n",
    "        pages  = driver.find_element_by_xpath(\"//span[@class='-pageInfo']/span[@class='-totalPages']\").text\n",
    "        pages = int(pages)\n",
    "\n",
    "        # Iterate the pages to scrape the values\n",
    "        for page in range (1, pages+1):\n",
    "\n",
    "            # Turn the page to BeautifulSoup\n",
    "            pagesource = driver.page_source\n",
    "            soup = BeautifulSoup(pagesource,'html.parser')\n",
    "\n",
    "            # all rows\n",
    "            rows = soup.find_all('div', class_='rt-tr-group')\n",
    "            for row in rows:\n",
    "                row_list = []\n",
    "                # all cols\n",
    "                cols = row.find_all('div', class_='searchResults__cellContent--3WEWj')\n",
    "                for col in cols:\n",
    "                    value = col.text\n",
    "                    # Append the value\n",
    "                    row_list.append(value)\n",
    "                # Append the year\n",
    "                row_list.append(year)\n",
    "\n",
    "                # For 2017, the order of Change in rank (1000) and Employees is different\n",
    "                if year==2017:\n",
    "                    temp=row_list[8]\n",
    "                    row_list[8]=row_list[9]\n",
    "                    row_list[9]=temp\n",
    "                \n",
    "                data_list.append(row_list)\n",
    "\n",
    "            # Page Turner\n",
    "            if (pages!=1) and (page<pages):\n",
    "                next_page = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CLASS_NAME, \"-next\")))\n",
    "                next_page.click()\n",
    "\n",
    "    df=pd.DataFrame(data_list,columns=columns)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain three datasets(Overall, Female CEO, Founder CEO)\n",
    "df_all=web_scraping()\n",
    "df_female=web_scraping(femaleCEO)\n",
    "df_founder=web_scraping(ceoFounder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Back up the three datasets\n",
    "df_copy = df_all.copy()\n",
    "df_founder_copy=df_founder.copy()\n",
    "df_female_copy=df_female.copy()\n",
    "\n",
    "# Empty list\n",
    "founder_check=[]\n",
    "female_check=[]\n",
    "founder_list=[]\n",
    "female_list=[]\n",
    "\n",
    "# Obtain the rank and year of founder and female for further matching\n",
    "for item in zip(df_founder_copy['Rank'],df_founder_copy['Year']):\n",
    "    founder_list.append(item)\n",
    "for item in zip(df_female_copy['Rank'],df_female_copy['Year']):\n",
    "    female_list.append(item)\n",
    "\n",
    "# Iterate each row in overall dataframe\n",
    "for index in range(df_copy.shape[0]):\n",
    "\n",
    "    # obtain the rank and year for each row\n",
    "    check = tuple(df_copy.loc[index,['Rank','Year']])\n",
    "\n",
    "    # Bool check in Founder CEO\n",
    "    if check in founder_list:\n",
    "        founder_check.append(1)\n",
    "    else:\n",
    "        founder_check.append(0)\n",
    "    \n",
    "    # Bool check in Female CEO\n",
    "    if check in female_list:\n",
    "        female_check.append(1)\n",
    "    else:\n",
    "        female_check.append(0)\n",
    "\n",
    "df_copy['Female CEO'] = female_check\n",
    "df_copy['Founder CEO'] = founder_check\n",
    "df_copy.to_csv('Fortune500.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping more detail for only 2020(Another dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_header(rows,financials):\n",
    "    with open ('Fortune500-2.csv','w') as r:\n",
    "        for row in rows:\n",
    "            data = row.find_all('div')\n",
    "            heading = data[0].text\n",
    "            r.write(heading)\n",
    "            r.write(',')\n",
    "        for financial in financials:\n",
    "            heading = financial.find('div').text\n",
    "            if ',' in heading:\n",
    "                heading = heading.replace(',','')\n",
    "            r.write(heading)\n",
    "            r.write(',')\n",
    "        r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_data(rows,financials):\n",
    "     with open ('Fortune500-2.csv','a') as r:\n",
    "            for row in rows:\n",
    "                data = row.find_all('div')\n",
    "                heading = data[0].text\n",
    "                if heading == 'Website':\n",
    "                    col = row.a.text\n",
    "                else:\n",
    "                    col = data[-1].text\n",
    "\n",
    "                if ',' in col:\n",
    "                    col = col.replace(',','')\n",
    "                r.write(col)\n",
    "                r.write(',')\n",
    "                \n",
    "            for financial in financials:\n",
    "                value = financial.find('div', class_='dataTable__value--2wIAD').text\n",
    "                if ',' in value:\n",
    "                    value = value.replace(',','')\n",
    "                r.write(value)\n",
    "                r.write(',')\n",
    "            r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_failed_scrapes(failed_scrapes):\n",
    "    '''\n",
    "    This function shouldn't run with the new method. i.e. extracting url from 'next' pagination, instead of 'guessing' url from company name.\n",
    "    AT&T is just ATT,\n",
    "    Amazon.com is just Amazon\n",
    "    Tapestry is actually Coach'''\n",
    "\n",
    "    if os.path.exists('Fortune500-2-failed.csv'):\n",
    "        with open ('Fortune500-2-failed.csv','a') as r:\n",
    "            for company in failed_scrapes:\n",
    "                r.write(company)\n",
    "                r.write('\\n')\n",
    "        \n",
    "    else:\n",
    "        with open ('Fortune500-2-failed.csv','w') as r:\n",
    "            for company in failed_scrapes:\n",
    "                r.write(company)\n",
    "                r.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This method would be a lot faster if we store the urls in a list, and then use BS4 in future to extract \n",
    "directly from the html, as opposed to using Selenium to render each page. \n",
    "Sadly, we need the page to render to get the next url...\n",
    "However, this is still a better solution than just 'guessing' the url. It makes the method a lot more \n",
    "robust for changes in the different years.\n",
    "'''\n",
    "\n",
    "path = \"/Users/hongbinlin/Downloads/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "try:\n",
    "    failed_scrapes = []\n",
    "    company = 'Walmart'\n",
    "    webpage = \"https://fortune.com/company/{}/fortune500/\".format(company)\n",
    "    \n",
    "# This range will scrape n number of pages(max = 1000), \n",
    "# given that company variable is Walmart ie. 1st position\n",
    "    for i in range(1000):\n",
    "#         print(webpage)\n",
    "        driver.get(webpage)\n",
    "\n",
    "        # This is important to ensure that the webpage loads all of the data\n",
    "        try:\n",
    "            temp = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"info__wrapper--1CxpW\")))\n",
    "\n",
    "        except:\n",
    "#             This won't run as company variable is no longer in a list\n",
    "#             failed_scrapes.append(company)\n",
    "            continue\n",
    "            \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        information = soup.find('div', class_='info__wrapper--1CxpW')\n",
    "        rows = information.find_all('div', class_='info__row--7f9lE')\n",
    "        financials = soup.find_all('div', class_='dataTable__row--3ws_o')\n",
    "\n",
    "        if i == 0:\n",
    "            initialise_header(rows,financials)\n",
    "        \n",
    "        scrape_page_data(rows,financials)\n",
    "\n",
    "        pagination = soup.find('div', class_='companySinglePagination__paginationWrapper--2m5Dj')\n",
    "        urls = pagination.find_all('a', href=True)\n",
    "        next_page = urls[-1]['href']\n",
    "        webpage = next_page\n",
    "        \n",
    "finally:\n",
    "    driver.quit()\n",
    "#     store_failed_scrapes(failed_scrapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1597353941833"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}