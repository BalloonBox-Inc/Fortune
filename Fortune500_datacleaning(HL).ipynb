{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: @michaelbrink\n",
    "#Org: BalloonBox Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_type_transform(df_origin):\n",
    "\n",
    "    # Make a copy\n",
    "    df = df_origin.copy()\n",
    "\n",
    "    # The list of text features that don't need to clean\n",
    "    text_list=['Name','Country','Headquarters','Industry','CEO','Website','Company Type','Ticker']\n",
    "\n",
    "    # The list of all the features\n",
    "    feature_list=df.columns\n",
    "    \n",
    "    # For loop for cleaninn\n",
    "    for col in feature_list:\n",
    "        # Text data doesn't need to clean\n",
    "        if col in text_list:\n",
    "            pass\n",
    "        else:\n",
    "            # 1: The '-' value in '%' columns gonna be 0!\n",
    "            df[col] = df[col].replace(to_replace='[\\$,%]',value='',regex=True)\n",
    "\n",
    "            # 2: The '-' value in ($M) columns will be dropped\n",
    "            if ('%' in col) or ('Change' in col) or ('Return' in col):\n",
    "                df[col] = df[col].replace(to_replace='^-$',value='0',regex=True)\n",
    "            \n",
    "            # 3: Convert them into numeric values\n",
    "            df[col] = pd.to_numeric(df[col],errors='coerce')\n",
    "            \n",
    "            # # 4: Divide by 100 for all '%' columns\n",
    "            # if ('%' in col) or ('Return' in col):\n",
    "            #     df[col] = round(df[col]/100,5)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_by_quantile(df_origin,cols):\n",
    "    '''\n",
    "        df is the dataframe we want to process: DataFrame\n",
    "        cols is a list of columns we want to process: List\n",
    "    '''\n",
    "    # Make a copy of DataFrame\n",
    "    df = df_origin.copy()\n",
    "\n",
    "    # List contains the index of dropped rows\n",
    "    drop_list = []\n",
    "\n",
    "    for col in cols:\n",
    "        # drop the outliers based on boxplot\n",
    "        temp = list(df[col].quantile([0.25,0.75]))\n",
    "        Q1 = temp[0]\n",
    "        Q3 = temp[1]\n",
    "        IQR = Q3-Q1\n",
    "        minimum = Q1-1.5*IQR\n",
    "        maximum = Q3+1.5*IQR\n",
    "        # Append the index of dropped rows\n",
    "        low_outlier = df[df[col]<=minimum].index\n",
    "        high_outlier = df[df[col]>=maximum].index\n",
    "        for index in low_outlier:\n",
    "            drop_list.append(index)\n",
    "        for index in high_outlier:\n",
    "            drop_list.append(index)\n",
    "        \n",
    "        # Deduplicated index\n",
    "        drop_list = list(dict.fromkeys(drop_list))\n",
    "    \n",
    "    # Drop the outliers\n",
    "    df.drop(drop_list,axis=0,inplace=True)\n",
    "\n",
    "    # # Histogram\n",
    "    # plt.hist(df['Employees'],bins=5)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_buckets(df_origin,cols):\n",
    "    # Make a copy of origin dataset\n",
    "    df = df_origin.copy()\n",
    "    temp_df = pd.DataFrame()\n",
    "\n",
    "    # Create 5 equal size buckets\n",
    "    for col in cols:\n",
    "        col_value = df[col]\n",
    "        qcut_value, bins = pd.qcut(x=col_value,q=[0,0.2,0.4,0.6,0.8,1.0],labels=[1,2,3,4,5],retbins=True)\n",
    "        temp_df[col+'_cat'] = qcut_value\n",
    "        # print('The bins of {col} is : {bins}'.format(col=col,bins=bins))\n",
    "    \n",
    "    # Concat the dataFrame\n",
    "    df = pd.concat([df,temp_df],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies(df_origin,cols):\n",
    "    # Make a copy of origin dataset\n",
    "    df = df_origin.copy()\n",
    "    temp_df =pd.get_dummies(df[cols].astype('str'))\n",
    "    # Concat the dataFrame\n",
    "    df = pd.concat([df,temp_df],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the original datasets\n",
    "df_all_origin=pd.read_csv('Fortune500.csv')\n",
    "df_detail_origin=pd.read_csv('Fortune500-2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The shape of original df_all is : (4000, 14)\nThe shape of df_all after dropping null value is :  (3774, 14)\nThe shape of df_all after dropping outliers is :  (2801, 14)\nThe shape of df_all after creating buckets is :  (2801, 18)\nThe shape of df_all after getting dummies is :  (2801, 41)\n"
    }
   ],
   "source": [
    "# Back up the original dataset\n",
    "df_all = df_all_origin.copy()\n",
    "print('The shape of original df_all is :',df_all.shape)\n",
    "\n",
    "\n",
    "# Data type transformation for df_all dataset\n",
    "df_all = data_type_transform(df_all)\n",
    "\n",
    "\n",
    "# Drop null values\n",
    "if df_all.isna().sum().sum() != 0:\n",
    "    df_all.dropna(inplace=True)\n",
    "print('The shape of df_all after dropping null value is : ',df_all.shape)\n",
    "\n",
    "\n",
    "# Drop outliers\n",
    "drop_cols = ['Profit ($M)','Assets ($M)','Market Value ($M)','Employees']\n",
    "df_all = drop_by_quantile(df_all,drop_cols)\n",
    "print('The shape of df_all after dropping outliers is : ',df_all.shape)\n",
    "\n",
    "\n",
    "# Create three buckets for Revenue(1:Low, 2:Medium, 3:High)\n",
    "conditions = [df_all['Revenue ($M)']<5000,(df_all['Revenue ($M)']>=5000)&(df_all['Revenue ($M)']<=10000),df_all['Revenue ($M)']>10000]\n",
    "values = [1,2,3]\n",
    "df_all['Revenue_cat'] = np.select(conditions,values)\n",
    "\n",
    "\n",
    "# Combine filter's columns together(0: No filter, 1: Female CEO, 2: Founder CEO, 3: Both)\n",
    "conditions = [(df_all['Female CEO']==0)&(df_all['Founder CEO']==0),\\\n",
    "              (df_all['Female CEO']==1)&(df_all['Founder CEO']==0),\\\n",
    "              (df_all['Female CEO']==0)&(df_all['Founder CEO']==1),\\\n",
    "              (df_all['Female CEO']==1)&(df_all['Founder CEO']==1)]\n",
    "values = [0,1,2,3]\n",
    "df_all['Filter_cat'] = np.select(conditions,values)\n",
    "if 'Female CEO' or 'Founder CEO' in df_all.columns:\n",
    "    df_all.drop(columns=['Female CEO','Founder CEO'],inplace=True)\n",
    "\n",
    "\n",
    "# Create buckets\n",
    "process_list = ['Profit ($M)','Assets ($M)','Market Value ($M)','Employees']\n",
    "df_all = create_buckets(df_all,process_list)\n",
    "print('The shape of df_all after creating buckets is : ',df_all.shape)\n",
    "\n",
    "\n",
    "# Get dummies\n",
    "process_list = ['Filter_cat','Profit ($M)_cat','Assets ($M)_cat','Market Value ($M)_cat','Employees_cat']\n",
    "df_all = dummies(df_all,process_list)\n",
    "print('The shape of df_all after getting dummies is : ',df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the csv files\n",
    "df_all.to_csv('Fortune500_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The shape of original df_detail is : (1000, 26)\nThe shape of df_detail after dropping duplicated columns is :  (1000, 23)\nThe shape of df_detail after dropping null value is :  (924, 23)\nThe shape of df_detail after dropping outliers is :  (621, 23)\nThe shape of df_all after creating buckets is :  (621, 30)\nThe shape of df_all after getting dummies is :  (621, 65)\n"
    }
   ],
   "source": [
    "# Back up the original dataset\n",
    "df_detail = df_detail_origin.copy()\n",
    "print('The shape of original df_detail is :',df_detail.shape)\n",
    "\n",
    "\n",
    "# firstly, drop the duplicated and useless columns in df_detail\n",
    "feature_list = df_detail.columns\n",
    "drop_list = ['Revenues ($M).1','Profits ($M).1','Unnamed: 25']\n",
    "for feature in feature_list:\n",
    "    if feature in drop_list:\n",
    "        df_detail.drop(columns=feature,inplace=True)\n",
    "print('The shape of df_detail after dropping duplicated columns is : ',df_detail.shape)\n",
    "\n",
    "\n",
    "# Data type transformation for df_all dataset\n",
    "df_detail = data_type_transform(df_detail)\n",
    "\n",
    "\n",
    "# Drop null values\n",
    "if df_detail.isna().sum().sum() != 0:\n",
    "    df_detail.dropna(inplace=True)\n",
    "print('The shape of df_detail after dropping null value is : ',df_detail.shape)\n",
    "\n",
    "\n",
    "# Drop outliers\n",
    "drop_cols = ['Profits ($M)','Market Value ($M)','Employees','Assets ($M)',\\\n",
    "             'Total Stockholder Equity ($M)','Earnings Per Share ($)']\n",
    "df_detail = drop_by_quantile(df_detail,drop_cols)\n",
    "print('The shape of df_detail after dropping outliers is : ',df_detail.shape)\n",
    "\n",
    "\n",
    "# Create buckets\n",
    "process_list = ['Revenues ($M)','Profits ($M)','Market Value ($M)','Employees','Assets ($M)',\\\n",
    "                'Total Stockholder Equity ($M)','Earnings Per Share ($)']\n",
    "df_detail = create_buckets(df_detail,process_list)\n",
    "print('The shape of df_all after creating buckets is : ',df_detail.shape)\n",
    "\n",
    "\n",
    "# Get dummies\n",
    "process_list = ['Revenues ($M)_cat','Profits ($M)_cat','Market Value ($M)_cat','Employees_cat','Assets ($M)_cat',\\\n",
    "                'Total Stockholder Equity ($M)_cat','Earnings Per Share ($)_cat']\n",
    "df_detail = dummies(df_detail,process_list)\n",
    "print('The shape of df_all after getting dummies is : ',df_detail.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the csv files\n",
    "df_detail.to_csv('Fortune500-2_cleaned.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python_defaultSpec_1597866959569"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}